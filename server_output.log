INFO:     Started server process [23054]
INFO:     Waiting for application startup.
{"timestamp": "2025-11-11T19:25:04.838036Z", "level": "info", "event": "Starting Boeing RAG Service..."}
{"pdf_path": "data/document_analysis/Boeing B737 Manual-1.pdf", "index_dir": "faiss_index", "top_k": 15, "rerank_top_k": 10, "timestamp": "2025-11-11T19:25:04.838148Z", "level": "info", "event": "Configuration loaded"}
{"timestamp": "2025-11-11T19:25:04.838621Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T19:25:04.838662Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T19:25:04.838691Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T19:25:04.839467Z", "level": "info", "event": "YAML config loaded"}
{"model": "models/text-embedding-004", "timestamp": "2025-11-11T19:25:04.839503Z", "level": "info", "event": "Loading embedding model"}
{"index_dir": "faiss_index", "timestamp": "2025-11-11T19:25:04.863861Z", "level": "info", "event": "VectorStoreManager initialized"}
{"index_dir": "faiss_index", "timestamp": "2025-11-11T19:25:04.863968Z", "level": "info", "event": "Loading existing vector store"}
Loading faiss.
Successfully loaded faiss.
Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
{"timestamp": "2025-11-11T19:25:04.888221Z", "level": "info", "event": "Vector store loaded successfully"}
{"timestamp": "2025-11-11T19:25:04.888262Z", "level": "info", "event": "Using existing vector store"}
{"timestamp": "2025-11-11T19:25:04.888284Z", "level": "info", "event": "Initializing Hybrid RAG Service (BM25 + Dense + Reranking)..."}
{"timestamp": "2025-11-11T19:25:04.888425Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T19:25:04.888450Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T19:25:04.888470Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T19:25:04.888846Z", "level": "info", "event": "YAML config loaded"}
{"provider": "google", "model": "models/gemini-2.0-pro-exp", "timestamp": "2025-11-11T19:25:04.888873Z", "level": "info", "event": "Loading LLM"}
{"timestamp": "2025-11-11T19:25:04.890962Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T19:25:04.890998Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T19:25:04.891022Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T19:25:04.891788Z", "level": "info", "event": "YAML config loaded"}
{"provider": "google", "model": "models/gemini-2.0-pro-exp", "timestamp": "2025-11-11T19:25:04.891820Z", "level": "info", "event": "Loading LLM"}
{"timestamp": "2025-11-11T19:25:04.892170Z", "level": "info", "event": "QueryExpander initialized"}
{"num_documents": 497, "timestamp": "2025-11-11T19:25:04.900670Z", "level": "info", "event": "BM25 index built"}
Use pytorch device: mps
{"timestamp": "2025-11-11T19:25:10.846248Z", "level": "info", "event": "Cross-encoder reranker loaded successfully"}
{"timestamp": "2025-11-11T19:25:10.848791Z", "level": "info", "event": "Hybrid RAG chain built successfully"}
{"top_k": 15, "rerank_top_k": 10, "timestamp": "2025-11-11T19:25:10.848864Z", "level": "info", "event": "HybridBoeingRAGService initialized"}
{"timestamp": "2025-11-11T19:25:10.848928Z", "level": "info", "event": "Boeing RAG Service started successfully"}
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:56619 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:56689 - "GET /health HTTP/1.1" 200 OK
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "timestamp": "2025-11-11T19:26:41.291370Z", "level": "info", "event": "Processing query request"}
{"question": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the OAT is 50\u00b0C. What's the climb limit weight ?", "timestamp": "2025-11-11T19:26:41.291821Z", "level": "info", "event": "Processing query"}
{"original_len": 144, "expanded_len": 322, "timestamp": "2025-11-11T19:26:41.320456Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "I'm calculating our takeoff weight for a dry runwa", "expanded_len": 322, "timestamp": "2025-11-11T19:26:41.320545Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 28, "timestamp": "2025-11-11T19:26:41.845172Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:26:42.626607Z", "level": "info", "event": "Reranking complete"}
Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 16
}
].
{"error": "429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 14\n}\n]", "question": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the OAT is 50\u00b0C. What's the climb limit weight ?", "timestamp": "2025-11-11T19:26:45.058203Z", "level": "error", "event": "Failed to process query"}
{"error": "Error in [/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py] at line [77] | Message: Query processing failed: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 14\n}\n]\nTraceback:\nTraceback (most recent call last):\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 330, in query\n    result = self.chain.invoke(question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 511, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 497, in _invoke\n    **self.mapper.invoke(\n      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in <dictcomp>\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3757, in _invoke_step\n    return context.run(\n           ^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4775, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4633, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 302, in <lambda>\n    ).invoke({\n      ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 980, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 799, in generate\n    self._generate_with_cache(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1045, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n    next_sleep = _retry_error_helper(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 14\n}\n]\n", "timestamp": "2025-11-11T19:26:45.067816Z", "level": "error", "event": "DocumentPortalException during query"}
INFO:     127.0.0.1:56691 - "POST /query HTTP/1.1" 500 Internal Server Error
{"question_preview": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retractio", "timestamp": "2025-11-11T19:26:45.579827Z", "level": "info", "event": "Processing query request"}
{"question": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retraction, and at what speed?", "timestamp": "2025-11-11T19:26:45.580079Z", "level": "info", "event": "Processing query"}
{"original_len": 121, "expanded_len": 250, "timestamp": "2025-11-11T19:26:45.580957Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "We're doing a Flaps 15 takeoff. Remind me, what is", "expanded_len": 250, "timestamp": "2025-11-11T19:26:45.581032Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 24, "timestamp": "2025-11-11T19:26:45.939842Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  9.69it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  9.67it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:26:46.119487Z", "level": "info", "event": "Reranking complete"}
Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 13
}
].
{"error": "429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 11\n}\n]", "question": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retraction, and at what speed?", "timestamp": "2025-11-11T19:26:48.498265Z", "level": "error", "event": "Failed to process query"}
{"error": "Error in [/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py] at line [77] | Message: Query processing failed: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 11\n}\n]\nTraceback:\nTraceback (most recent call last):\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 330, in query\n    result = self.chain.invoke(question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 511, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 497, in _invoke\n    **self.mapper.invoke(\n      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in <dictcomp>\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3757, in _invoke_step\n    return context.run(\n           ^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4775, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4633, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 302, in <lambda>\n    ).invoke({\n      ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 980, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 799, in generate\n    self._generate_with_cache(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1045, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n    next_sleep = _retry_error_helper(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 11\n}\n]\n", "timestamp": "2025-11-11T19:26:48.500947Z", "level": "error", "event": "DocumentPortalException during query"}
INFO:     127.0.0.1:56706 - "POST /query HTTP/1.1" 500 Internal Server Error
{"question_preview": "We're planning a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude airport. If the ", "timestamp": "2025-11-11T19:26:49.018521Z", "level": "info", "event": "Processing query request"}
{"question": "We're planning a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude airport. If the wind-corrected field length is 1,600 meters, what is our field limit weight?", "timestamp": "2025-11-11T19:26:49.018632Z", "level": "info", "event": "Processing query"}
{"original_len": 176, "expanded_len": 390, "timestamp": "2025-11-11T19:26:49.019092Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "We're planning a Flaps 40 landing on a wet runway ", "expanded_len": 390, "timestamp": "2025-11-11T19:26:49.019139Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 26, "timestamp": "2025-11-11T19:26:49.472901Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.04it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:26:49.983328Z", "level": "info", "event": "Reranking complete"}
Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 9
}
].
{"error": "429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 7\n}\n]", "question": "We're planning a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude airport. If the wind-corrected field length is 1,600 meters, what is our field limit weight?", "timestamp": "2025-11-11T19:26:52.309789Z", "level": "error", "event": "Failed to process query"}
{"error": "Error in [/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py] at line [77] | Message: Query processing failed: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 7\n}\n]\nTraceback:\nTraceback (most recent call last):\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 330, in query\n    result = self.chain.invoke(question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 511, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 497, in _invoke\n    **self.mapper.invoke(\n      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in <dictcomp>\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3757, in _invoke_step\n    return context.run(\n           ^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4775, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4633, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 302, in <lambda>\n    ).invoke({\n      ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 980, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 799, in generate\n    self._generate_with_cache(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1045, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n    next_sleep = _retry_error_helper(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 7\n}\n]\n", "timestamp": "2025-11-11T19:26:52.311834Z", "level": "error", "event": "DocumentPortalException during query"}
INFO:     127.0.0.1:56720 - "POST /query HTTP/1.1" 500 Internal Server Error
{"question_preview": "Reviewing the standard takeoff profile: After we're airborne and get a positive rate of climb, what ", "timestamp": "2025-11-11T19:26:52.823729Z", "level": "info", "event": "Processing query request"}
{"question": "Reviewing the standard takeoff profile: After we're airborne and get a positive rate of climb, what is the first action we take?", "timestamp": "2025-11-11T19:26:52.823903Z", "level": "info", "event": "Processing query"}
{"original_len": 128, "expanded_len": 175, "timestamp": "2025-11-11T19:26:52.824359Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "Reviewing the standard takeoff profile: After we'r", "expanded_len": 175, "timestamp": "2025-11-11T19:26:52.824401Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 26, "timestamp": "2025-11-11T19:26:53.386556Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.72it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:26:53.752757Z", "level": "info", "event": "Reranking complete"}
Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 5
}
].
{"error": "429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 3\n}\n]", "question": "Reviewing the standard takeoff profile: After we're airborne and get a positive rate of climb, what is the first action we take?", "timestamp": "2025-11-11T19:26:56.018100Z", "level": "error", "event": "Failed to process query"}
{"error": "Error in [/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py] at line [77] | Message: Query processing failed: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 3\n}\n]\nTraceback:\nTraceback (most recent call last):\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 330, in query\n    result = self.chain.invoke(question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 511, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 497, in _invoke\n    **self.mapper.invoke(\n      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in <dictcomp>\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3757, in _invoke_step\n    return context.run(\n           ^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4775, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4633, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 302, in <lambda>\n    ).invoke({\n      ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 980, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 799, in generate\n    self._generate_with_cache(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1045, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n    next_sleep = _retry_error_helper(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 3\n}\n]\n", "timestamp": "2025-11-11T19:26:56.019793Z", "level": "error", "event": "DocumentPortalException during query"}
INFO:     127.0.0.1:56731 - "POST /query HTTP/1.1" 500 Internal Server Error
{"question_preview": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible f", "timestamp": "2025-11-11T19:26:56.531991Z", "level": "info", "event": "Processing query request"}
{"question": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible for the forward aisle stand?", "timestamp": "2025-11-11T19:26:56.532102Z", "level": "info", "event": "Processing query"}
{"original_len": 127, "expanded_len": 127, "timestamp": "2025-11-11T19:26:56.532400Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "Looking at the panel scan responsibilities for whe", "expanded_len": 127, "timestamp": "2025-11-11T19:26:56.532429Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 27, "timestamp": "2025-11-11T19:26:56.843932Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.43it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:26:57.359353Z", "level": "info", "event": "Reranking complete"}
Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 2
}
].
{"error": "429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n}\n]", "question": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible for the forward aisle stand?", "timestamp": "2025-11-11T19:26:59.680138Z", "level": "error", "event": "Failed to process query"}
{"error": "Error in [/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py] at line [77] | Message: Query processing failed: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n}\n]\nTraceback:\nTraceback (most recent call last):\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 330, in query\n    result = self.chain.invoke(question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 511, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 497, in _invoke\n    **self.mapper.invoke(\n      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in <dictcomp>\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3757, in _invoke_step\n    return context.run(\n           ^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4775, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4633, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 302, in <lambda>\n    ).invoke({\n      ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 980, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 799, in generate\n    self._generate_with_cache(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1045, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n    next_sleep = _retry_error_helper(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n}\n]\n", "timestamp": "2025-11-11T19:26:59.683710Z", "level": "error", "event": "DocumentPortalException during query"}
INFO:     127.0.0.1:56745 - "POST /query HTTP/1.1" 500 Internal Server Error
{"question_preview": "For a standard visual pattern, what three actions must be completed prior to turning base?", "timestamp": "2025-11-11T19:27:00.200748Z", "level": "info", "event": "Processing query request"}
{"question": "For a standard visual pattern, what three actions must be completed prior to turning base?", "timestamp": "2025-11-11T19:27:00.200872Z", "level": "info", "event": "Processing query"}
{"original_len": 90, "expanded_len": 90, "timestamp": "2025-11-11T19:27:00.201487Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "For a standard visual pattern, what three actions ", "expanded_len": 90, "timestamp": "2025-11-11T19:27:00.201521Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 30, "timestamp": "2025-11-11T19:27:00.591094Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:27:01.537492Z", "level": "info", "event": "Reranking complete"}
Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 58
}
].
{"error": "429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 56\n}\n]", "question": "For a standard visual pattern, what three actions must be completed prior to turning base?", "timestamp": "2025-11-11T19:27:03.799414Z", "level": "error", "event": "Failed to process query"}
{"error": "Error in [/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py] at line [77] | Message: Query processing failed: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 56\n}\n]\nTraceback:\nTraceback (most recent call last):\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 330, in query\n    result = self.chain.invoke(question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 511, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 497, in _invoke\n    **self.mapper.invoke(\n      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in <dictcomp>\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3757, in _invoke_step\n    return context.run(\n           ^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4775, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4633, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 302, in <lambda>\n    ).invoke({\n      ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 980, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 799, in generate\n    self._generate_with_cache(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1045, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n    next_sleep = _retry_error_helper(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 56\n}\n]\n", "timestamp": "2025-11-11T19:27:03.802123Z", "level": "error", "event": "DocumentPortalException during query"}
INFO:     127.0.0.1:56760 - "POST /query HTTP/1.1" 500 Internal Server Error
{"question_preview": "If the PF is making entries into the CDU during flight, what must the PF do prior to execution?", "timestamp": "2025-11-11T19:27:04.329051Z", "level": "info", "event": "Processing query request"}
{"question": "If the PF is making entries into the CDU during flight, what must the PF do prior to execution?", "timestamp": "2025-11-11T19:27:04.345744Z", "level": "info", "event": "Processing query"}
{"original_len": 95, "expanded_len": 95, "timestamp": "2025-11-11T19:27:04.347885Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "If the PF is making entries into the CDU during fl", "expanded_len": 95, "timestamp": "2025-11-11T19:27:04.348341Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 28, "timestamp": "2025-11-11T19:27:04.880008Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  4.26it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  4.26it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:27:05.506859Z", "level": "info", "event": "Reranking complete"}
Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 54
}
].
{"error": "429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 52\n}\n]", "question": "If the PF is making entries into the CDU during flight, what must the PF do prior to execution?", "timestamp": "2025-11-11T19:27:07.799756Z", "level": "error", "event": "Failed to process query"}
{"error": "Error in [/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py] at line [77] | Message: Query processing failed: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 52\n}\n]\nTraceback:\nTraceback (most recent call last):\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 330, in query\n    result = self.chain.invoke(question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 511, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 497, in _invoke\n    **self.mapper.invoke(\n      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in <dictcomp>\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3757, in _invoke_step\n    return context.run(\n           ^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4775, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4633, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 302, in <lambda>\n    ).invoke({\n      ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 980, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 799, in generate\n    self._generate_with_cache(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1045, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n    next_sleep = _retry_error_helper(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 52\n}\n]\n", "timestamp": "2025-11-11T19:27:07.801978Z", "level": "error", "event": "DocumentPortalException during query"}
INFO:     127.0.0.1:56774 - "POST /query HTTP/1.1" 500 Internal Server Error
{"question_preview": "I see an amber \"STAIRS OPER\" light illuminated on the forward attendant panel; what does that light ", "timestamp": "2025-11-11T19:27:08.311449Z", "level": "info", "event": "Processing query request"}
{"question": "I see an amber \"STAIRS OPER\" light illuminated on the forward attendant panel; what does that light indicate?", "timestamp": "2025-11-11T19:27:08.311590Z", "level": "info", "event": "Processing query"}
{"original_len": 109, "expanded_len": 109, "timestamp": "2025-11-11T19:27:08.311864Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "I see an amber \"STAIRS OPER\" light illuminated on ", "expanded_len": 109, "timestamp": "2025-11-11T19:27:08.311903Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 29, "timestamp": "2025-11-11T19:27:08.681285Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  7.64it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:27:08.966602Z", "level": "info", "event": "Reranking complete"}
Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 50
}
].
{"error": "429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 48\n}\n]", "question": "I see an amber \"STAIRS OPER\" light illuminated on the forward attendant panel; what does that light indicate?", "timestamp": "2025-11-11T19:27:11.253740Z", "level": "error", "event": "Failed to process query"}
{"error": "Error in [/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py] at line [77] | Message: Query processing failed: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 48\n}\n]\nTraceback:\nTraceback (most recent call last):\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 330, in query\n    result = self.chain.invoke(question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 511, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 497, in _invoke\n    **self.mapper.invoke(\n      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in <dictcomp>\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3757, in _invoke_step\n    return context.run(\n           ^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4775, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4633, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 302, in <lambda>\n    ).invoke({\n      ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 980, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 799, in generate\n    self._generate_with_cache(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1045, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n    next_sleep = _retry_error_helper(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 48\n}\n]\n", "timestamp": "2025-11-11T19:27:11.257520Z", "level": "error", "event": "DocumentPortalException during query"}
INFO:     127.0.0.1:56788 - "POST /query HTTP/1.1" 500 Internal Server Error
{"question_preview": "We've just completed the engine start. What is the correct configuration for the ISOLATION VALVE swi", "timestamp": "2025-11-11T19:27:11.771552Z", "level": "info", "event": "Processing query request"}
{"question": "We've just completed the engine start. What is the correct configuration for the ISOLATION VALVE switch during the After Start Procedure?", "timestamp": "2025-11-11T19:27:11.771737Z", "level": "info", "event": "Processing query"}
{"original_len": 137, "expanded_len": 215, "timestamp": "2025-11-11T19:27:11.772914Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "We've just completed the engine start. What is the", "expanded_len": 215, "timestamp": "2025-11-11T19:27:11.772977Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 22, "timestamp": "2025-11-11T19:27:12.126388Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  5.48it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  5.48it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:27:12.637914Z", "level": "info", "event": "Reranking complete"}
Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 47
}
].
{"error": "429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 44\n}\n]", "question": "We've just completed the engine start. What is the correct configuration for the ISOLATION VALVE switch during the After Start Procedure?", "timestamp": "2025-11-11T19:27:14.942034Z", "level": "error", "event": "Failed to process query"}
{"error": "Error in [/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py] at line [77] | Message: Query processing failed: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 44\n}\n]\nTraceback:\nTraceback (most recent call last):\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 330, in query\n    result = self.chain.invoke(question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 511, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 497, in _invoke\n    **self.mapper.invoke(\n      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in <dictcomp>\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3757, in _invoke_step\n    return context.run(\n           ^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4775, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4633, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 302, in <lambda>\n    ).invoke({\n      ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 980, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 799, in generate\n    self._generate_with_cache(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1045, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n    next_sleep = _retry_error_helper(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 44\n}\n]\n", "timestamp": "2025-11-11T19:27:14.948686Z", "level": "error", "event": "DocumentPortalException during query"}
INFO:     127.0.0.1:56800 - "POST /query HTTP/1.1" 500 Internal Server Error
{"question_preview": "During the Descent and Approach procedure, what action is taken with the AUTO BRAKE select switch , ", "timestamp": "2025-11-11T19:27:15.467953Z", "level": "info", "event": "Processing query request"}
{"question": "During the Descent and Approach procedure, what action is taken with the AUTO BRAKE select switch , and what is the Pilot Flying's final action regarding the autobrake system during the Landing Roll procedure?", "timestamp": "2025-11-11T19:27:15.468065Z", "level": "info", "event": "Processing query"}
{"original_len": 209, "expanded_len": 287, "timestamp": "2025-11-11T19:27:15.468773Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "During the Descent and Approach procedure, what ac", "expanded_len": 287, "timestamp": "2025-11-11T19:27:15.468815Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 27, "timestamp": "2025-11-11T19:27:15.742085Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  4.29it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:27:16.049047Z", "level": "info", "event": "Reranking complete"}
Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {
  description: "Learn more about Gemini API quotas"
  url: "https://ai.google.dev/gemini-api/docs/rate-limits"
}
, violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count"
  quota_id: "GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerMinutePerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
violations {
  quota_metric: "generativelanguage.googleapis.com/generate_content_paid_tier_2_requests"
  quota_id: "GenerateRequestsPerDayPerProjectPerModel-PaidTier2"
  quota_dimensions {
    key: "model"
    value: "gemini-2.0-pro-exp"
  }
  quota_dimensions {
    key: "location"
    value: "global"
  }
}
, retry_delay {
  seconds: 43
}
].
{"error": "429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 41\n}\n]", "question": "During the Descent and Approach procedure, what action is taken with the AUTO BRAKE select switch , and what is the Pilot Flying's final action regarding the autobrake system during the Landing Roll procedure?", "timestamp": "2025-11-11T19:27:18.324963Z", "level": "error", "event": "Failed to process query"}
{"error": "Error in [/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py] at line [77] | Message: Query processing failed: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 41\n}\n]\nTraceback:\nTraceback (most recent call last):\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 330, in query\n    result = self.chain.invoke(question)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 511, in invoke\n    return self._call_with_config(self._invoke, input, config, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/passthrough.py\", line 497, in _invoke\n    **self.mapper.invoke(\n      ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in invoke\n    output = {key: future.result() for key, future in zip(steps, futures)}\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3773, in <dictcomp>\n    output = {key: future.result() for key, future in zip(steps, futures)}\n                   ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3757, in _invoke_step\n    return context.run(\n           ^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4775, in invoke\n    return self._call_with_config(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 1939, in _call_with_config\n    context.run(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 4633, in _invoke\n    output = call_func_with_variable_args(\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 429, in call_func_with_variable_args\n    return func(input, **kwargs)  # type: ignore[call-arg]\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /src/rag_service.py\", line 302, in <lambda>\n    ).invoke({\n      ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3046, in invoke\n    input_ = context.run(step.invoke, input_, config)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n    self.generate_prompt(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 980, in generate_prompt\n    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 799, in generate\n    self._generate_with_cache(\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1045, in _generate_with_cache\n    result = self._generate(\n             ^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 961, in _generate\n    response: GenerateContentResponse = _chat_with_retry(\n                                        ^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 196, in _chat_with_retry\n    return _chat_with_retry(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 338, in wrapped_f\n    return copy(f, *args, **kw)\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 477, in __call__\n    do = self.iter(retry_state=retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 378, in iter\n    result = action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 420, in exc_check\n    raise retry_exc.reraise()\n          ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 187, in reraise\n    raise self.last_attempt.result()\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/tenacity/__init__.py\", line 480, in __call__\n    result = fn(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 194, in _chat_with_retry\n    raise e\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/langchain_google_genai/chat_models.py\", line 178, in _chat_with_retry\n    return generation_method(**kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 835, in generate_content\n    response = rpc(\n               ^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n    return wrapped_func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n    return retry_target(\n           ^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n    next_sleep = _retry_error_helper(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n    raise final_exc from source_exc\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n    result = target()\n             ^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/site-packages/google/api_core/grpc_helpers.py\", line 77, in error_remapped_callable\n    raise exceptions.from_grpc_error(exc) from exc\ngoogle.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota. Please migrate to Gemini 2.5 Pro Preview (models/gemini-2.5-pro-preview-03-25) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n, violations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_requests\"\n  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\nviolations {\n  quota_metric: \"generativelanguage.googleapis.com/generate_content_paid_tier_2_input_token_count\"\n  quota_id: \"GenerateContentPaidTierInputTokensPerModelPerMinute-PaidTier2\"\n  quota_dimensions {\n    key: \"model\"\n    value: \"gemini-2.0-pro-exp\"\n  }\n  quota_dimensions {\n    key: \"location\"\n    value: \"global\"\n  }\n}\n, retry_delay {\n  seconds: 41\n}\n]\n", "timestamp": "2025-11-11T19:27:18.327645Z", "level": "error", "event": "DocumentPortalException during query"}
INFO:     127.0.0.1:56814 - "POST /query HTTP/1.1" 500 Internal Server Error
/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO:     Started server process [24470]
INFO:     Waiting for application startup.
{"timestamp": "2025-11-11T19:27:58.480730Z", "level": "info", "event": "Starting Boeing RAG Service..."}
{"pdf_path": "data/document_analysis/Boeing B737 Manual-1.pdf", "index_dir": "faiss_index", "top_k": 15, "rerank_top_k": 10, "timestamp": "2025-11-11T19:27:58.480804Z", "level": "info", "event": "Configuration loaded"}
{"timestamp": "2025-11-11T19:27:58.481250Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T19:27:58.481283Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T19:27:58.481309Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T19:27:58.481784Z", "level": "info", "event": "YAML config loaded"}
{"model": "models/text-embedding-004", "timestamp": "2025-11-11T19:27:58.481814Z", "level": "info", "event": "Loading embedding model"}
{"index_dir": "faiss_index", "timestamp": "2025-11-11T19:27:58.488756Z", "level": "info", "event": "VectorStoreManager initialized"}
{"index_dir": "faiss_index", "timestamp": "2025-11-11T19:27:58.488846Z", "level": "info", "event": "Loading existing vector store"}
Loading faiss.
Successfully loaded faiss.
Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
{"timestamp": "2025-11-11T19:27:58.502974Z", "level": "info", "event": "Vector store loaded successfully"}
{"timestamp": "2025-11-11T19:27:58.503032Z", "level": "info", "event": "Using existing vector store"}
{"timestamp": "2025-11-11T19:27:58.503061Z", "level": "info", "event": "Initializing Hybrid RAG Service (BM25 + Dense + Reranking)..."}
{"timestamp": "2025-11-11T19:27:58.503257Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T19:27:58.503300Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T19:27:58.503333Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T19:27:58.503772Z", "level": "info", "event": "YAML config loaded"}
{"provider": "google", "model": "models/gemini-2.5-pro-preview-03-25", "timestamp": "2025-11-11T19:27:58.503812Z", "level": "info", "event": "Loading LLM"}
{"timestamp": "2025-11-11T19:27:58.505638Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T19:27:58.505673Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T19:27:58.505697Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T19:27:58.506592Z", "level": "info", "event": "YAML config loaded"}
{"provider": "google", "model": "models/gemini-2.5-pro-preview-03-25", "timestamp": "2025-11-11T19:27:58.506626Z", "level": "info", "event": "Loading LLM"}
{"timestamp": "2025-11-11T19:27:58.506989Z", "level": "info", "event": "QueryExpander initialized"}
{"num_documents": 497, "timestamp": "2025-11-11T19:27:58.515432Z", "level": "info", "event": "BM25 index built"}
Use pytorch device: mps
{"timestamp": "2025-11-11T19:28:03.377557Z", "level": "info", "event": "Cross-encoder reranker loaded successfully"}
{"timestamp": "2025-11-11T19:28:03.378878Z", "level": "info", "event": "Hybrid RAG chain built successfully"}
{"top_k": 15, "rerank_top_k": 10, "timestamp": "2025-11-11T19:28:03.378954Z", "level": "info", "event": "HybridBoeingRAGService initialized"}
{"timestamp": "2025-11-11T19:28:03.379001Z", "level": "info", "event": "Boeing RAG Service started successfully"}
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:57182 - "GET /docs HTTP/1.1" 200 OK
INFO:     127.0.0.1:57182 - "GET /openapi.json HTTP/1.1" 200 OK
INFO:     127.0.0.1:57196 - "GET /docs HTTP/1.1" 200 OK
INFO:     127.0.0.1:57196 - "GET /openapi.json HTTP/1.1" 200 OK
INFO:     127.0.0.1:57277 - "POST /query HTTP/1.1" 422 Unprocessable Entity
INFO:     127.0.0.1:57284 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:57359 - "GET /health HTTP/1.1" 200 OK
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "timestamp": "2025-11-11T19:28:54.317882Z", "level": "info", "event": "Processing query request"}
{"question": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the OAT is 50\u00b0C. What's the climb limit weight ?", "timestamp": "2025-11-11T19:28:54.318307Z", "level": "info", "event": "Processing query"}
{"original_len": 144, "expanded_len": 322, "timestamp": "2025-11-11T19:28:54.336020Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "I'm calculating our takeoff weight for a dry runwa", "expanded_len": 322, "timestamp": "2025-11-11T19:28:54.336172Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 28, "timestamp": "2025-11-11T19:28:54.927372Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:28:55.510590Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 532, "answer_preview": "Based on the provided manual pages, the tables are titled \"Takeoff Field & Climb Limit Weights\" (Pages 82, 83). The procedure states to enter the appropriate table and \"Also read Climb Limit Weight fo", "timestamp": "2025-11-11T19:29:07.593952Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 3, "pages": [82, 83, 104], "timestamp": "2025-11-11T19:29:07.595038Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "num_pages": 3, "pages": [82, 83, 104], "timestamp": "2025-11-11T19:29:07.595136Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 3, "answer_preview": "Based on the provided manual pages, the tables are titled \"Takeoff Field & Climb Limit Weights\" . The procedure states to enter the appropriate table ", "timestamp": "2025-11-11T19:29:07.595247Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:57361 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retractio", "timestamp": "2025-11-11T19:29:08.111363Z", "level": "info", "event": "Processing query request"}
{"question": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retraction, and at what speed?", "timestamp": "2025-11-11T19:29:08.111560Z", "level": "info", "event": "Processing query"}
{"original_len": 121, "expanded_len": 250, "timestamp": "2025-11-11T19:29:08.112241Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "We're doing a Flaps 15 takeoff. Remind me, what is", "expanded_len": 250, "timestamp": "2025-11-11T19:29:08.112306Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 24, "timestamp": "2025-11-11T19:29:08.394561Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:29:08.859121Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 164, "answer_preview": "Based on the provided manual pages, for a Flaps 15 takeoff, the first flap selection during retraction is to **select Flaps 5** at a speed of **V2 + 15** (Page 41).", "timestamp": "2025-11-11T19:29:15.407667Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [41], "timestamp": "2025-11-11T19:29:15.408529Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retractio", "num_pages": 1, "pages": [41], "timestamp": "2025-11-11T19:29:15.408592Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "Based on the provided manual pages, for a Flaps 15 takeoff, the first flap selection during retraction is to **select Flaps 5** at a speed of **V2 + 1", "timestamp": "2025-11-11T19:29:15.408671Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:57404 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "We're planning a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude airport. If the ", "timestamp": "2025-11-11T19:29:15.926355Z", "level": "info", "event": "Processing query request"}
{"question": "We're planning a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude airport. If the wind-corrected field length is 1,600 meters, what is our field limit weight?", "timestamp": "2025-11-11T19:29:15.931932Z", "level": "info", "event": "Processing query"}
{"original_len": 176, "expanded_len": 390, "timestamp": "2025-11-11T19:29:15.932408Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "We're planning a Flaps 40 landing on a wet runway ", "expanded_len": 390, "timestamp": "2025-11-11T19:29:15.932449Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 26, "timestamp": "2025-11-11T19:29:16.237849Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.81it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.81it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:29:16.755888Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 201, "answer_preview": "Based on the provided manual, for a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude with a wind-corrected field length of 1,600 meters, the field limit weight is 55,800 kg (Page 99)", "timestamp": "2025-11-11T19:29:25.605361Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [99], "timestamp": "2025-11-11T19:29:25.614365Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "We're planning a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude airport. If the ", "num_pages": 1, "pages": [99], "timestamp": "2025-11-11T19:29:25.614543Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "Based on the provided manual, for a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude with a wind-corrected field length of 1,600 met", "timestamp": "2025-11-11T19:29:25.615263Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:57437 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "Reviewing the standard takeoff profile: After we're airborne and get a positive rate of climb, what ", "timestamp": "2025-11-11T19:29:26.128968Z", "level": "info", "event": "Processing query request"}
{"question": "Reviewing the standard takeoff profile: After we're airborne and get a positive rate of climb, what is the first action we take?", "timestamp": "2025-11-11T19:29:26.129106Z", "level": "info", "event": "Processing query"}
{"original_len": 128, "expanded_len": 175, "timestamp": "2025-11-11T19:29:26.129569Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "Reviewing the standard takeoff profile: After we'r", "expanded_len": 175, "timestamp": "2025-11-11T19:29:26.129620Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 26, "timestamp": "2025-11-11T19:29:26.441774Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.66it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:29:26.958131Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 313, "answer_preview": "Based on the provided manual context, when a positive rate of climb is indicated after becoming airborne, the first action taken by the Pilot Flying is to call \u201cGEAR UP\u201d (Page 39).\n\nFollowing this cal", "timestamp": "2025-11-11T19:29:38.755108Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [39], "timestamp": "2025-11-11T19:29:38.755903Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "Reviewing the standard takeoff profile: After we're airborne and get a positive rate of climb, what ", "num_pages": 1, "pages": [39], "timestamp": "2025-11-11T19:29:38.755948Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "Based on the provided manual context, when a positive rate of climb is indicated after becoming airborne, the first action taken by the Pilot Flying i", "timestamp": "2025-11-11T19:29:38.756019Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:57470 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible f", "timestamp": "2025-11-11T19:29:39.275043Z", "level": "info", "event": "Processing query request"}
{"question": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible for the forward aisle stand?", "timestamp": "2025-11-11T19:29:39.275294Z", "level": "info", "event": "Processing query"}
{"original_len": 127, "expanded_len": 127, "timestamp": "2025-11-11T19:29:39.276187Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "Looking at the panel scan responsibilities for whe", "expanded_len": 127, "timestamp": "2025-11-11T19:29:39.276283Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 27, "timestamp": "2025-11-11T19:29:39.551033Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.07it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:29:40.073153Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 359, "answer_preview": "Based on the provided manual pages, when the aircraft is not moving under its own power, responsibility for the forward aisle stand is split between the Captain and the First Officer.\n\nThe \"Panel Scan", "timestamp": "2025-11-11T19:29:48.958640Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [6], "timestamp": "2025-11-11T19:29:48.959623Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible f", "num_pages": 1, "pages": [6], "timestamp": "2025-11-11T19:29:48.959677Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "Based on the provided manual pages, when the aircraft is not moving under its own power, responsibility for the forward aisle stand is split between t", "timestamp": "2025-11-11T19:29:48.959766Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:57511 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "For a standard visual pattern, what three actions must be completed prior to turning base?", "timestamp": "2025-11-11T19:29:49.483644Z", "level": "info", "event": "Processing query request"}
{"question": "For a standard visual pattern, what three actions must be completed prior to turning base?", "timestamp": "2025-11-11T19:29:49.483857Z", "level": "info", "event": "Processing query"}
{"original_len": 90, "expanded_len": 90, "timestamp": "2025-11-11T19:29:49.484520Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "For a standard visual pattern, what three actions ", "expanded_len": 90, "timestamp": "2025-11-11T19:29:49.484623Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 30, "timestamp": "2025-11-11T19:29:49.789533Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.90it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.89it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:29:50.280033Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 232, "answer_preview": "Based on the provided manual, the three primary actions to be completed prior to turning base in a visual traffic pattern are:\n\n*   Gear down (Page 56)\n*   Flaps 15 (landing flaps for 1 engine) (Page ", "timestamp": "2025-11-11T19:29:59.601603Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [56], "timestamp": "2025-11-11T19:29:59.610983Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "For a standard visual pattern, what three actions must be completed prior to turning base?", "num_pages": 1, "pages": [56], "timestamp": "2025-11-11T19:29:59.611072Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "Based on the provided manual, the three primary actions to be completed prior to turning base in a visual traffic pattern are: * Gear down * Flaps 15 ", "timestamp": "2025-11-11T19:29:59.611264Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:57543 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "If the PF is making entries into the CDU during flight, what must the PF do prior to execution?", "timestamp": "2025-11-11T19:30:00.135743Z", "level": "info", "event": "Processing query request"}
{"question": "If the PF is making entries into the CDU during flight, what must the PF do prior to execution?", "timestamp": "2025-11-11T19:30:00.135950Z", "level": "info", "event": "Processing query"}
{"original_len": 95, "expanded_len": 95, "timestamp": "2025-11-11T19:30:00.136816Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "If the PF is making entries into the CDU during fl", "expanded_len": 95, "timestamp": "2025-11-11T19:30:00.136905Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 28, "timestamp": "2025-11-11T19:30:00.437029Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:30:01.133401Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 204, "answer_preview": "Based on the provided context, the normal procedure is for the pilot not flying (PNF) to make the CDU entries. These entries must then be **verified by the pilot flying (PF) prior to execution** (Page", "timestamp": "2025-11-11T19:30:10.234242Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [5], "timestamp": "2025-11-11T19:30:10.235241Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "If the PF is making entries into the CDU during flight, what must the PF do prior to execution?", "num_pages": 1, "pages": [5], "timestamp": "2025-11-11T19:30:10.235301Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "Based on the provided context, the normal procedure is for the pilot not flying (PNF) to make the CDU entries. These entries must then be **verified b", "timestamp": "2025-11-11T19:30:10.235440Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:57578 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "I see an amber \"STAIRS OPER\" light illuminated on the forward attendant panel; what does that light ", "timestamp": "2025-11-11T19:30:10.757502Z", "level": "info", "event": "Processing query request"}
{"question": "I see an amber \"STAIRS OPER\" light illuminated on the forward attendant panel; what does that light indicate?", "timestamp": "2025-11-11T19:30:10.757668Z", "level": "info", "event": "Processing query"}
{"original_len": 109, "expanded_len": 109, "timestamp": "2025-11-11T19:30:10.758336Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "I see an amber \"STAIRS OPER\" light illuminated on ", "expanded_len": 109, "timestamp": "2025-11-11T19:30:10.758415Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 29, "timestamp": "2025-11-11T19:30:11.037828Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.29it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:30:11.629635Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 134, "answer_preview": "Based on the provided manual, an illuminated amber STAIRS Operating (OPER) light indicates that the airstair is in transit (Page 126).", "timestamp": "2025-11-11T19:30:18.330501Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [126], "timestamp": "2025-11-11T19:30:18.331418Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "I see an amber \"STAIRS OPER\" light illuminated on the forward attendant panel; what does that light ", "num_pages": 1, "pages": [126], "timestamp": "2025-11-11T19:30:18.331467Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "Based on the provided manual, an illuminated amber STAIRS Operating (OPER) light indicates that the airstair is in transit .", "timestamp": "2025-11-11T19:30:18.331550Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:57610 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "We've just completed the engine start. What is the correct configuration for the ISOLATION VALVE swi", "timestamp": "2025-11-11T19:30:18.848006Z", "level": "info", "event": "Processing query request"}
{"question": "We've just completed the engine start. What is the correct configuration for the ISOLATION VALVE switch during the After Start Procedure?", "timestamp": "2025-11-11T19:30:18.848190Z", "level": "info", "event": "Processing query"}
{"original_len": 137, "expanded_len": 215, "timestamp": "2025-11-11T19:30:18.848875Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "We've just completed the engine start. What is the", "expanded_len": 215, "timestamp": "2025-11-11T19:30:18.848948Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 22, "timestamp": "2025-11-11T19:30:19.149182Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.02it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:30:19.888820Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 138, "answer_preview": "Based on the manual provided, the correct configuration for the ISOLATION VALVE switch during the After Start Procedure is AUTO (Page 35).", "timestamp": "2025-11-11T19:30:26.843201Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [35], "timestamp": "2025-11-11T19:30:26.845232Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "We've just completed the engine start. What is the correct configuration for the ISOLATION VALVE swi", "num_pages": 1, "pages": [35], "timestamp": "2025-11-11T19:30:26.845321Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "Based on the manual provided, the correct configuration for the ISOLATION VALVE switch during the After Start Procedure is AUTO .", "timestamp": "2025-11-11T19:30:26.845714Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:57636 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "During the Descent and Approach procedure, what action is taken with the AUTO BRAKE select switch , ", "timestamp": "2025-11-11T19:30:27.362365Z", "level": "info", "event": "Processing query request"}
{"question": "During the Descent and Approach procedure, what action is taken with the AUTO BRAKE select switch , and what is the Pilot Flying's final action regarding the autobrake system during the Landing Roll procedure?", "timestamp": "2025-11-11T19:30:27.362535Z", "level": "info", "event": "Processing query"}
{"original_len": 209, "expanded_len": 287, "timestamp": "2025-11-11T19:30:27.363217Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "During the Descent and Approach procedure, what ac", "expanded_len": 287, "timestamp": "2025-11-11T19:30:27.363302Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 27, "timestamp": "2025-11-11T19:30:27.697272Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.30it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T19:30:28.069402Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 326, "answer_preview": "Based on the provided manual pages:\n\n*   During the Descent and Approach procedure, the Pilot Not Flying is to \"Set AUTO BRAKE select switch to desired brake setting\" (Page 43).\n*   During the Landing", "timestamp": "2025-11-11T19:30:36.425589Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 2, "pages": [43, 47], "timestamp": "2025-11-11T19:30:36.426601Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "During the Descent and Approach procedure, what action is taken with the AUTO BRAKE select switch , ", "num_pages": 2, "pages": [43, 47], "timestamp": "2025-11-11T19:30:36.426670Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 2, "answer_preview": "Based on the provided manual pages: * During the Descent and Approach procedure, the Pilot Not Flying is to \"Set AUTO BRAKE select switch to desired b", "timestamp": "2025-11-11T19:30:36.426783Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:57666 - "POST /query HTTP/1.1" 200 OK
/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO:     Started server process [34446]
INFO:     Waiting for application startup.
{"timestamp": "2025-11-11T20:01:26.830914Z", "level": "info", "event": "Starting Boeing RAG Service..."}
{"pdf_path": "data/document_analysis/Boeing B737 Manual-1.pdf", "index_dir": "faiss_index", "top_k": 15, "rerank_top_k": 10, "timestamp": "2025-11-11T20:01:26.831007Z", "level": "info", "event": "Configuration loaded"}
{"timestamp": "2025-11-11T20:01:26.831342Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T20:01:26.831382Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T20:01:26.831412Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T20:01:26.832190Z", "level": "info", "event": "YAML config loaded"}
{"model": "models/text-embedding-004", "timestamp": "2025-11-11T20:01:26.832252Z", "level": "info", "event": "Loading embedding model"}
{"index_dir": "faiss_index", "timestamp": "2025-11-11T20:01:26.849791Z", "level": "info", "event": "VectorStoreManager initialized"}
{"index_dir": "faiss_index", "timestamp": "2025-11-11T20:01:26.849908Z", "level": "info", "event": "Loading existing vector store"}
Loading faiss.
Successfully loaded faiss.
Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
{"timestamp": "2025-11-11T20:01:26.887329Z", "level": "info", "event": "Vector store loaded successfully"}
{"timestamp": "2025-11-11T20:01:26.887389Z", "level": "info", "event": "Using existing vector store"}
{"timestamp": "2025-11-11T20:01:26.887415Z", "level": "info", "event": "Initializing Hybrid RAG Service (BM25 + Dense + Reranking)..."}
{"timestamp": "2025-11-11T20:01:26.887603Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T20:01:26.887635Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T20:01:26.887660Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T20:01:26.888110Z", "level": "info", "event": "YAML config loaded"}
{"provider": "google", "model": "models/gemini-2.5-pro-preview-03-25", "timestamp": "2025-11-11T20:01:26.888146Z", "level": "info", "event": "Loading LLM"}
{"timestamp": "2025-11-11T20:01:26.890502Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T20:01:26.890571Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T20:01:26.890599Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T20:01:26.891737Z", "level": "info", "event": "YAML config loaded"}
{"provider": "google", "model": "models/gemini-2.5-pro-preview-03-25", "timestamp": "2025-11-11T20:01:26.891800Z", "level": "info", "event": "Loading LLM"}
{"timestamp": "2025-11-11T20:01:26.892501Z", "level": "info", "event": "QueryExpander initialized"}
{"num_documents": 497, "timestamp": "2025-11-11T20:01:26.901897Z", "level": "info", "event": "BM25 index built"}
Use pytorch device: mps
{"timestamp": "2025-11-11T20:01:32.003739Z", "level": "info", "event": "Cross-encoder reranker loaded successfully"}
{"timestamp": "2025-11-11T20:01:32.006145Z", "level": "info", "event": "Hybrid RAG chain built successfully"}
{"top_k": 15, "rerank_top_k": 10, "timestamp": "2025-11-11T20:01:32.006214Z", "level": "info", "event": "HybridBoeingRAGService initialized"}
{"timestamp": "2025-11-11T20:01:32.006268Z", "level": "info", "event": "Boeing RAG Service started successfully"}
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:64586 - "GET /health HTTP/1.1" 200 OK
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "timestamp": "2025-11-11T20:02:29.342685Z", "level": "info", "event": "Processing query request"}
{"question": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the OAT is 50\u00b0C. What's the climb limit weight?", "timestamp": "2025-11-11T20:02:29.343171Z", "level": "info", "event": "Processing query"}
{"original_len": 143, "expanded_len": 321, "timestamp": "2025-11-11T20:02:29.354754Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "I'm calculating our takeoff weight for a dry runwa", "expanded_len": 321, "timestamp": "2025-11-11T20:02:29.354873Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 28, "timestamp": "2025-11-11T20:02:29.811623Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:02:30.602007Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 0, "answer_preview": "EMPTY", "timestamp": "2025-11-11T20:02:50.530155Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 0, "pages": [], "timestamp": "2025-11-11T20:02:50.531156Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "num_pages": 0, "pages": [], "timestamp": "2025-11-11T20:02:50.531257Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 0, "answer_preview": "", "timestamp": "2025-11-11T20:02:50.531397Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:64723 - "POST /query HTTP/1.1" 200 OK
INFO:     127.0.0.1:64966 - "GET / HTTP/1.1" 200 OK
INFO:     127.0.0.1:64966 - "GET /favicon.ico HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:64974 - "GET /docs HTTP/1.1" 200 OK
INFO:     127.0.0.1:64974 - "GET /openapi.json HTTP/1.1" 200 OK
{"question_preview": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible f", "timestamp": "2025-11-11T20:05:13.928930Z", "level": "info", "event": "Processing query request"}
{"question": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible for the forward aisle stand?", "timestamp": "2025-11-11T20:05:13.932284Z", "level": "info", "event": "Processing query"}
{"original_len": 127, "expanded_len": 127, "timestamp": "2025-11-11T20:05:13.956276Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "Looking at the panel scan responsibilities for whe", "expanded_len": 127, "timestamp": "2025-11-11T20:05:13.956706Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 27, "timestamp": "2025-11-11T20:05:14.264411Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]Batches: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:05:15.611239Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 197, "answer_preview": "When the aircraft is stationary, responsibility for the forward aisle stand is shared. The Captain is responsible for the left side and the First Officer is responsible for the right side (Page 6).", "timestamp": "2025-11-11T20:05:23.788071Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [6], "timestamp": "2025-11-11T20:05:23.791724Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible f", "num_pages": 1, "pages": [6], "timestamp": "2025-11-11T20:05:23.792175Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "When the aircraft is stationary, responsibility for the forward aisle stand is shared. The Captain is responsible for the left side and the First Offi", "timestamp": "2025-11-11T20:05:23.792606Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:64982 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "timestamp": "2025-11-11T20:11:05.725437Z", "level": "info", "event": "Processing query request"}
{"question": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the OAT is 50\u00b0C. What's the climb limit weight ?", "timestamp": "2025-11-11T20:11:05.726827Z", "level": "info", "event": "Processing query"}
{"original_len": 144, "expanded_len": 322, "timestamp": "2025-11-11T20:11:05.732033Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "I'm calculating our takeoff weight for a dry runwa", "expanded_len": 322, "timestamp": "2025-11-11T20:11:05.732274Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 28, "timestamp": "2025-11-11T20:11:06.361878Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]Batches: 100%|██████████| 1/1 [00:01<00:00,  1.33s/it]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:11:07.913475Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 67, "answer_preview": "The Climb Limit Weight for those conditions is 52,200 kg (Page 83).", "timestamp": "2025-11-11T20:11:16.235263Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [83], "timestamp": "2025-11-11T20:11:16.236688Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "num_pages": 1, "pages": [83], "timestamp": "2025-11-11T20:11:16.236790Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "The Climb Limit Weight for those conditions is 52,200 kg .", "timestamp": "2025-11-11T20:11:16.237197Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51658 - "POST /query HTTP/1.1" 200 OK
INFO:     127.0.0.1:51737 - "GET /health HTTP/1.1" 200 OK
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "timestamp": "2025-11-11T20:16:25.549970Z", "level": "info", "event": "Processing query request"}
{"question": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the OAT is 50\u00b0C. What's the climb limit weight ?", "timestamp": "2025-11-11T20:16:25.550626Z", "level": "info", "event": "Processing query"}
{"original_len": 144, "expanded_len": 322, "timestamp": "2025-11-11T20:16:25.556899Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "I'm calculating our takeoff weight for a dry runwa", "expanded_len": 322, "timestamp": "2025-11-11T20:16:25.557024Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 28, "timestamp": "2025-11-11T20:16:26.080231Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]Batches: 100%|██████████| 1/1 [00:01<00:00,  1.63s/it]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:16:27.926621Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 0, "answer_preview": "EMPTY", "timestamp": "2025-11-11T20:16:49.714939Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 0, "pages": [], "timestamp": "2025-11-11T20:16:49.716568Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "num_pages": 0, "pages": [], "timestamp": "2025-11-11T20:16:49.716728Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 0, "answer_preview": "", "timestamp": "2025-11-11T20:16:49.716988Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51739 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retractio", "timestamp": "2025-11-11T20:16:50.245229Z", "level": "info", "event": "Processing query request"}
{"question": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retraction, and at what speed?", "timestamp": "2025-11-11T20:16:50.245513Z", "level": "info", "event": "Processing query"}
{"original_len": 121, "expanded_len": 250, "timestamp": "2025-11-11T20:16:50.246582Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "We're doing a Flaps 15 takeoff. Remind me, what is", "expanded_len": 250, "timestamp": "2025-11-11T20:16:50.246693Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 24, "timestamp": "2025-11-11T20:16:51.063209Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.39it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:16:51.557800Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 81, "answer_preview": "After a Flaps 15 takeoff, the first selection is to Flaps 5 at V2 + 15 (Page 41).", "timestamp": "2025-11-11T20:16:58.715847Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [41], "timestamp": "2025-11-11T20:16:58.716890Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retractio", "num_pages": 1, "pages": [41], "timestamp": "2025-11-11T20:16:58.717040Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "After a Flaps 15 takeoff, the first selection is to Flaps 5 at V2 + 15 .", "timestamp": "2025-11-11T20:16:58.717164Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51745 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "We're planning a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude airport. If the ", "timestamp": "2025-11-11T20:16:59.239396Z", "level": "info", "event": "Processing query request"}
{"question": "We're planning a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude airport. If the wind-corrected field length is 1,600 meters, what is our field limit weight?", "timestamp": "2025-11-11T20:16:59.239728Z", "level": "info", "event": "Processing query"}
{"original_len": 176, "expanded_len": 390, "timestamp": "2025-11-11T20:16:59.240876Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "We're planning a Flaps 40 landing on a wet runway ", "expanded_len": 390, "timestamp": "2025-11-11T20:16:59.241028Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 26, "timestamp": "2025-11-11T20:16:59.530176Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.52it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:17:00.097222Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 144, "answer_preview": "For a wet runway at 1,000 feet pressure altitude with a 1,600 meter wind-corrected field length, your field limit weight is 55,800 kg (Page 99).", "timestamp": "2025-11-11T20:17:09.589275Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [99], "timestamp": "2025-11-11T20:17:09.590962Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "We're planning a Flaps 40 landing on a wet runway at a 1,000-foot pressure altitude airport. If the ", "num_pages": 1, "pages": [99], "timestamp": "2025-11-11T20:17:09.591174Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "For a wet runway at 1,000 feet pressure altitude with a 1,600 meter wind-corrected field length, your field limit weight is 55,800 kg .", "timestamp": "2025-11-11T20:17:09.591394Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51748 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "Reviewing the standard takeoff profile: After we're airborne and get a positive rate of climb, what ", "timestamp": "2025-11-11T20:17:10.114471Z", "level": "info", "event": "Processing query request"}
{"question": "Reviewing the standard takeoff profile: After we're airborne and get a positive rate of climb, what is the first action we take?", "timestamp": "2025-11-11T20:17:10.114833Z", "level": "info", "event": "Processing query"}
{"original_len": 128, "expanded_len": 175, "timestamp": "2025-11-11T20:17:10.116111Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "Reviewing the standard takeoff profile: After we'r", "expanded_len": 175, "timestamp": "2025-11-11T20:17:10.116307Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 26, "timestamp": "2025-11-11T20:17:13.229932Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.65it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:17:13.592765Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 43, "answer_preview": "The Pilot Flying calls \u201cGEAR UP\u201d (Page 39).", "timestamp": "2025-11-11T20:17:19.909744Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [39], "timestamp": "2025-11-11T20:17:19.910368Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "Reviewing the standard takeoff profile: After we're airborne and get a positive rate of climb, what ", "num_pages": 1, "pages": [39], "timestamp": "2025-11-11T20:17:19.910449Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "The Pilot Flying calls \u201cGEAR UP\u201d .", "timestamp": "2025-11-11T20:17:19.910540Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51750 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible f", "timestamp": "2025-11-11T20:17:20.424770Z", "level": "info", "event": "Processing query request"}
{"question": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible for the forward aisle stand?", "timestamp": "2025-11-11T20:17:20.425317Z", "level": "info", "event": "Processing query"}
{"original_len": 127, "expanded_len": 127, "timestamp": "2025-11-11T20:17:20.426811Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "Looking at the panel scan responsibilities for whe", "expanded_len": 127, "timestamp": "2025-11-11T20:17:20.427033Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 27, "timestamp": "2025-11-11T20:17:20.707218Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.52it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.49it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:17:21.214821Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 359, "answer_preview": "When the aircraft is stationary, responsibility for the forward aisle stand is split between the Captain and the First Officer.\n\nThe Captain is responsible for the left side, and the First Officer is ", "timestamp": "2025-11-11T20:17:32.225210Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [6], "timestamp": "2025-11-11T20:17:32.226421Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "Looking at the panel scan responsibilities for when the aircraft is stationary, who is responsible f", "num_pages": 1, "pages": [6], "timestamp": "2025-11-11T20:17:32.226685Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "When the aircraft is stationary, responsibility for the forward aisle stand is split between the Captain and the First Officer. The Captain is respons", "timestamp": "2025-11-11T20:17:32.226891Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51752 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "For a standard visual pattern, what three actions must be completed prior to turning base?", "timestamp": "2025-11-11T20:17:32.757020Z", "level": "info", "event": "Processing query request"}
{"question": "For a standard visual pattern, what three actions must be completed prior to turning base?", "timestamp": "2025-11-11T20:17:32.757387Z", "level": "info", "event": "Processing query"}
{"original_len": 90, "expanded_len": 90, "timestamp": "2025-11-11T20:17:32.760177Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "For a standard visual pattern, what three actions ", "expanded_len": 90, "timestamp": "2025-11-11T20:17:32.760374Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 30, "timestamp": "2025-11-11T20:17:33.127791Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.74it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:17:33.637435Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 158, "answer_preview": "Prior to turning base in a visual pattern, you need to accomplish these three configuration changes:\n\n*   Gear down\n*   Flaps 15\n*   Arm speedbrake\n\n(Page 56)", "timestamp": "2025-11-11T20:17:47.068534Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [56], "timestamp": "2025-11-11T20:17:47.071417Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "For a standard visual pattern, what three actions must be completed prior to turning base?", "num_pages": 1, "pages": [56], "timestamp": "2025-11-11T20:17:47.071509Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "Prior to turning base in a visual pattern, you need to accomplish these three configuration changes: * Gear down * Flaps 15 * Arm speedbrake", "timestamp": "2025-11-11T20:17:47.071896Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51754 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "If the PF is making entries into the CDU during flight, what must the PF do prior to execution?", "timestamp": "2025-11-11T20:17:47.608931Z", "level": "info", "event": "Processing query request"}
{"question": "If the PF is making entries into the CDU during flight, what must the PF do prior to execution?", "timestamp": "2025-11-11T20:17:47.609163Z", "level": "info", "event": "Processing query"}
{"original_len": 95, "expanded_len": 95, "timestamp": "2025-11-11T20:17:47.610139Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "If the PF is making entries into the CDU during fl", "expanded_len": 95, "timestamp": "2025-11-11T20:17:47.610236Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 28, "timestamp": "2025-11-11T20:17:49.525495Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.68it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:17:50.144102Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 153, "answer_preview": "In flight, CDU entries are normally accomplished by the Pilot Not Flying (PNF) and must be verified by the Pilot Flying (PF) prior to execution (Page 5).", "timestamp": "2025-11-11T20:18:00.571205Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [5], "timestamp": "2025-11-11T20:18:00.573402Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "If the PF is making entries into the CDU during flight, what must the PF do prior to execution?", "num_pages": 1, "pages": [5], "timestamp": "2025-11-11T20:18:00.573587Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "In flight, CDU entries are normally accomplished by the Pilot Not Flying (PNF) and must be verified by the Pilot Flying (PF) prior to execution .", "timestamp": "2025-11-11T20:18:00.573846Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51757 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "I see an amber \"STAIRS OPER\" light illuminated on the forward attendant panel; what does that light ", "timestamp": "2025-11-11T20:18:01.096564Z", "level": "info", "event": "Processing query request"}
{"question": "I see an amber \"STAIRS OPER\" light illuminated on the forward attendant panel; what does that light indicate?", "timestamp": "2025-11-11T20:18:01.096961Z", "level": "info", "event": "Processing query"}
{"original_len": 109, "expanded_len": 109, "timestamp": "2025-11-11T20:18:01.097991Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "I see an amber \"STAIRS OPER\" light illuminated on ", "expanded_len": 109, "timestamp": "2025-11-11T20:18:01.098177Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 29, "timestamp": "2025-11-11T20:18:01.421027Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  3.90it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:18:02.011200Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 65, "answer_preview": "That amber light indicates the airstair is in transit (Page 126).", "timestamp": "2025-11-11T20:18:07.226464Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [126], "timestamp": "2025-11-11T20:18:07.227664Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "I see an amber \"STAIRS OPER\" light illuminated on the forward attendant panel; what does that light ", "num_pages": 1, "pages": [126], "timestamp": "2025-11-11T20:18:07.227886Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "That amber light indicates the airstair is in transit .", "timestamp": "2025-11-11T20:18:07.228055Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51761 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "We've just completed the engine start. What is the correct configuration for the ISOLATION VALVE swi", "timestamp": "2025-11-11T20:18:07.746115Z", "level": "info", "event": "Processing query request"}
{"question": "We've just completed the engine start. What is the correct configuration for the ISOLATION VALVE switch during the After Start Procedure?", "timestamp": "2025-11-11T20:18:07.746697Z", "level": "info", "event": "Processing query"}
{"original_len": 137, "expanded_len": 215, "timestamp": "2025-11-11T20:18:07.748285Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "We've just completed the engine start. What is the", "expanded_len": 215, "timestamp": "2025-11-11T20:18:07.748505Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 22, "timestamp": "2025-11-11T20:18:08.365001Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  4.39it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:18:08.955715Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 68, "answer_preview": "The ISOLATION VALVE switch should be in the AUTO position (Page 35).", "timestamp": "2025-11-11T20:18:13.577578Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 1, "pages": [35], "timestamp": "2025-11-11T20:18:13.578595Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "We've just completed the engine start. What is the correct configuration for the ISOLATION VALVE swi", "num_pages": 1, "pages": [35], "timestamp": "2025-11-11T20:18:13.578837Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 1, "answer_preview": "The ISOLATION VALVE switch should be in the AUTO position .", "timestamp": "2025-11-11T20:18:13.579016Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51763 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "During the Descent and Approach procedure, what action is taken with the AUTO BRAKE select switch , ", "timestamp": "2025-11-11T20:18:14.097931Z", "level": "info", "event": "Processing query request"}
{"question": "During the Descent and Approach procedure, what action is taken with the AUTO BRAKE select switch , and what is the Pilot Flying's final action regarding the autobrake system during the Landing Roll procedure?", "timestamp": "2025-11-11T20:18:14.098296Z", "level": "info", "event": "Processing query"}
{"original_len": 209, "expanded_len": 287, "timestamp": "2025-11-11T20:18:14.099262Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "During the Descent and Approach procedure, what ac", "expanded_len": 287, "timestamp": "2025-11-11T20:18:14.099435Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 27, "timestamp": "2025-11-11T20:18:14.383517Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  4.01it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  4.01it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T20:18:14.700685Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 234, "answer_preview": "During the descent and approach, the Pilot Not Flying will set the AUTO BRAKE select switch to the desired brake setting (Page 43).\n\nDuring the landing roll, the Pilot Flying's action is to verify pro", "timestamp": "2025-11-11T20:18:24.016640Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 2, "pages": [43, 47], "timestamp": "2025-11-11T20:18:24.017756Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "During the Descent and Approach procedure, what action is taken with the AUTO BRAKE select switch , ", "num_pages": 2, "pages": [43, 47], "timestamp": "2025-11-11T20:18:24.017964Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 2, "answer_preview": "During the descent and approach, the Pilot Not Flying will set the AUTO BRAKE select switch to the desired brake setting . During the landing roll, th", "timestamp": "2025-11-11T20:18:24.018125Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:51766 - "POST /query HTTP/1.1" 200 OK
/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
INFO:     Started server process [45105]
INFO:     Waiting for application startup.
{"timestamp": "2025-11-11T20:53:02.881510Z", "level": "info", "event": "Starting Boeing RAG Service..."}
{"pdf_path": "data/document_analysis/Boeing B737 Manual-1.pdf", "index_dir": "faiss_index", "top_k": 15, "rerank_top_k": 10, "timestamp": "2025-11-11T20:53:02.881623Z", "level": "info", "event": "Configuration loaded"}
{"timestamp": "2025-11-11T20:53:02.882107Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T20:53:02.882146Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T20:53:02.882176Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T20:53:02.882950Z", "level": "info", "event": "YAML config loaded"}
{"model": "models/text-embedding-004", "timestamp": "2025-11-11T20:53:02.882993Z", "level": "info", "event": "Loading embedding model"}
{"index_dir": "faiss_index", "timestamp": "2025-11-11T20:53:02.902451Z", "level": "info", "event": "VectorStoreManager initialized"}
{"index_dir": "faiss_index", "timestamp": "2025-11-11T20:53:02.902557Z", "level": "info", "event": "Loading existing vector store"}
Loading faiss.
Successfully loaded faiss.
Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.
{"timestamp": "2025-11-11T20:53:02.933086Z", "level": "info", "event": "Vector store loaded successfully"}
{"timestamp": "2025-11-11T20:53:02.933166Z", "level": "info", "event": "Using existing vector store"}
{"timestamp": "2025-11-11T20:53:02.933193Z", "level": "info", "event": "Initializing Hybrid RAG Service (BM25 + Dense + Reranking)..."}
{"timestamp": "2025-11-11T20:53:02.933377Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T20:53:02.933408Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T20:53:02.933432Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T20:53:02.933873Z", "level": "info", "event": "YAML config loaded"}
{"provider": "google", "model": "models/gemini-2.5-pro-preview-03-25", "timestamp": "2025-11-11T20:53:02.933907Z", "level": "info", "event": "Loading LLM"}
{"timestamp": "2025-11-11T20:53:02.936000Z", "level": "info", "event": "Running in LOCAL mode: .env loaded"}
{"timestamp": "2025-11-11T20:53:02.936056Z", "level": "info", "event": "Loaded GOOGLE_API_KEY from individual env var"}
{"keys": {"GOOGLE_API_KEY": "AIzaSy..."}, "timestamp": "2025-11-11T20:53:02.936081Z", "level": "info", "event": "API keys loaded"}
{"config_keys": ["faiss_db", "embedding_model", "retriever", "llm"], "timestamp": "2025-11-11T20:53:02.937173Z", "level": "info", "event": "YAML config loaded"}
{"provider": "google", "model": "models/gemini-2.5-pro-preview-03-25", "timestamp": "2025-11-11T20:53:02.937215Z", "level": "info", "event": "Loading LLM"}
{"timestamp": "2025-11-11T20:53:02.937717Z", "level": "info", "event": "QueryExpander initialized"}
{"num_documents": 497, "timestamp": "2025-11-11T20:53:02.947180Z", "level": "info", "event": "BM25 index built"}
Use pytorch device: mps
{"timestamp": "2025-11-11T20:53:08.839923Z", "level": "info", "event": "Cross-encoder reranker loaded successfully"}
{"timestamp": "2025-11-11T20:53:08.841707Z", "level": "info", "event": "Hybrid RAG chain built successfully"}
{"top_k": 15, "rerank_top_k": 10, "timestamp": "2025-11-11T20:53:08.841773Z", "level": "info", "event": "HybridBoeingRAGService initialized"}
{"timestamp": "2025-11-11T20:53:08.841825Z", "level": "info", "event": "Boeing RAG Service started successfully"}
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:58338 - "GET /health HTTP/1.1" 200 OK
INFO:     127.0.0.1:63632 - "GET /health HTTP/1.1" 200 OK
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "timestamp": "2025-11-11T23:00:26.776151Z", "level": "info", "event": "Processing query request"}
{"question": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the OAT is 50\u00b0C. What's the climb limit weight ?", "timestamp": "2025-11-11T23:00:26.795924Z", "level": "info", "event": "Processing query"}
{"original_len": 144, "expanded_len": 322, "timestamp": "2025-11-11T23:00:26.975732Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "I'm calculating our takeoff weight for a dry runwa", "expanded_len": 322, "timestamp": "2025-11-11T23:00:26.976060Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 28, "timestamp": "2025-11-11T23:00:30.250355Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:03<00:00,  3.58s/it]Batches: 100%|██████████| 1/1 [00:03<00:00,  3.58s/it]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T23:00:34.069997Z", "level": "info", "event": "Reranking complete"}
{"answer_length": 347, "answer_preview": "The tables for Takeoff Field & Climb Limit Weights show the Field Limit Weight in the main body of the chart, which varies with runway length.\n\nWhile the manual states you should also read the Climb L", "timestamp": "2025-11-11T23:00:52.507984Z", "level": "info", "event": "Raw LLM answer received"}
{"num_pages": 0, "pages": [], "timestamp": "2025-11-11T23:00:52.510124Z", "level": "info", "event": "Extracted pages from answer"}
{"question_preview": "I'm calculating our takeoff weight for a dry runway. We're at 2,000 feet pressure altitude, and the ", "num_pages": 0, "pages": [], "timestamp": "2025-11-11T23:00:52.510222Z", "level": "info", "event": "Query processed successfully"}
{"num_pages": 0, "answer_preview": "The tables for Takeoff Field & Climb Limit Weights show the Field Limit Weight in the main body of the chart, which varies with runway length. While t", "timestamp": "2025-11-11T23:00:52.510352Z", "level": "info", "event": "Query processed successfully"}
INFO:     127.0.0.1:63634 - "POST /query HTTP/1.1" 200 OK
{"question_preview": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retractio", "timestamp": "2025-11-11T23:00:53.037852Z", "level": "info", "event": "Processing query request"}
{"question": "We're doing a Flaps 15 takeoff. Remind me, what is the first flap selection we make during retraction, and at what speed?", "timestamp": "2025-11-11T23:00:53.037964Z", "level": "info", "event": "Processing query"}
{"original_len": 121, "expanded_len": 250, "timestamp": "2025-11-11T23:00:53.038657Z", "level": "info", "event": "Query expanded with keywords"}
{"original": "We're doing a Flaps 15 takeoff. Remind me, what is", "expanded_len": 250, "timestamp": "2025-11-11T23:00:53.038711Z", "level": "info", "event": "Query expanded"}
{"dense_results": 15, "bm25_results": 15, "combined_results": 24, "timestamp": "2025-11-11T23:00:54.324934Z", "level": "info", "event": "Hybrid retrieval complete"}
Batches:   0%|          | 0/1 [00:00<?, ?it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]Batches: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]
{"original_docs": 15, "reranked_docs": 15, "timestamp": "2025-11-11T23:00:54.981787Z", "level": "info", "event": "Reranking complete"}
/Users/dhruvshrinet/Desktop/Assignment /env/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
